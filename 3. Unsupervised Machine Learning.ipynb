{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from dsiad_functions import Solution_UML\n",
    "from dsiad_functions import plots\n",
    "solution = Solution_UML()\n",
    "plot = plots()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous module we have prepared the data by imputing the mean for missing values, removing highly correlated features and removing features with very low variance. We will reuse our prepared dataset, by loading the data saved at the end of the previous module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = pd.read_csv(\"winequality-red_3.csv\") \n",
    "X = wine.drop([\"quality\"], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Unsupervised Machine Learning we only use the features, to look for hidden patterns in the data. We do not have a target or `y`. We set the features to `X`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Feature Engineering\n",
    "\n",
    "Before clustering, we normalize our dataset. This means that we set the mean of every column to zero and calculate each value relative to this mean. By doing this, each columns has the same range of values, which makes it more suitable for comparison.\n",
    "\n",
    "Let's first check our dataset with the function we have learned! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to apply normalization. Remember the formula for normalization is: (X - the minimum of X) / (the maximum of X - the minimum of X). \n",
    "\n",
    "You can use the following functions: \n",
    "`.min()` gives the minimum \n",
    "`.max()` gives the maximum\n",
    "\n",
    "View the difference between X and X_normalized afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized = \n",
    "\n",
    "X_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution.step_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5: Feature selection \n",
    "\n",
    "We allready selected our features! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6: Modelling\n",
    "\n",
    "## 6.1 Selecting the number of clusters\n",
    "\n",
    "When we use a k-means clustering technique, we should select the number of clusters beforehand. One way to do this is the elbow method. We want to look for the elbow, the point where the slope suddenly decreases. How to select the right number of clusters? A good cluster should have tight clusters, but not too many clusters. A simple rule of thumb is to find the elbow of the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.elbow_plot(X_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where do you think the elbow is? What is our optimal number of cluster? Test the results later for different numbers of clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_clusters = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the amount of clusters below. Fill out the number of clusters in the solution function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution.step_61(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 K-means clustering of the data\n",
    "\n",
    "Here, we define our model by the function `KMeans()` and fit the model to our dataset `X_normalized`. In the image you see an example of how the kmeans algorithm works with k = 3. \n",
    "\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"images/k-means.gif\" width=\"300\"><br/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = number_clusters).fit(X_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are using the fitted model to determine the cluster for each row of our data set by `.predict()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_pred = kmeans.predict(X_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add the cluster numbers to our (non-normalized) dataset. You can see a new colomn `cluster` that shows to which cluster the observation belongs based on our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine[\"cluster\"] = cluster_pred\n",
    "wine.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 7: Reviewing results\n",
    "## 7.1 Inspect centroids\n",
    "\n",
    "To see whether the clusters make sense, we can compare the values of the centroids of the different clusters. Remember, we want to see that next to compactness, isolation is optimized. This means that the centroids have different locations. Because we normalized the dataset, the location can be between 0 and 1. If one centroid is 0 and the other 1, than they have maximum distance. If one centroid is at 0.5 and the other at 0.51, they are fairly close and the clusters are highly likely to have overlap. Let's look at `kmean-cluster_centers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#returns the coordinates of the centers\n",
    "pd.DataFrame(kmeans.cluster_centers_ , columns = X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Inspect clusters in pairs \n",
    "It is not possible to visualize the clusters with all features at once. However, we can inspect the combination of 2 different features and see if the clusters are showing in the data. We do this with the use of `plt.scatter()` by the input argument `c=cluster-pred` we tell that we would like to have different colours for the different clusters. Inspect the clusters: would you define them as a cluster when seeing them visually? Try different combinations of features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_normalized.columns = X.columns\n",
    "\n",
    "x_column = \"alcohol\" ## CHANGE VARIABLE ##\n",
    "y_column = \"pH\"  ## CHANGE VARIABLE ##\n",
    "\n",
    "plt.scatter(X_normalized[x_column], X_normalized[y_column], c=cluster_pred)\n",
    "plt.xlabel(x_column)\n",
    "plt.ylabel(y_column)\n",
    "plt.title(\"Clustering of the wine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Characteristics of the wine per cluster\n",
    "\n",
    "We would like to inspect the difference in characteristics per cluster. With that information, we can determine what wine would suit which occasion! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_means = wine.groupby('cluster').mean().transpose()\n",
    "\n",
    "cluster_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_means.plot.barh()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
